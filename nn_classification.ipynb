{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00e045c",
   "metadata": {},
   "source": [
    "# Neural Network Classification \n",
    "\n",
    "This notebook contains the `ANN` class and helper functions converted from `src/nn_classification.py`.\n",
    "Use the cells below to inspect, run, and experiment with the neural-network code on CIFAR-10 batches included in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd625485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b15e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, layers_size,Lambda):\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    "        self.accuracy = []\n",
    "        self.accuracy_test = []\n",
    "        self.Lambda = Lambda #Regularization Parameter\n",
    "        self.beta1 = 0.9 #Adam almost parameter\n",
    "        self.beta2 = 0.99 #Adam almost parameter\n",
    "        \n",
    "    def sigmoid(self, Z):\n",
    "        Z = np.clip( Z, -500, 500 ) # Prevent overflow\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z)) # Shift the max value of Z to avoid the exponential to blow up\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        Z = np.clip( Z, -500, 500 ) # Prevent overflow\n",
    "        s = 1 / (1 + np.exp(-Z)) \n",
    "        return s * (1 - s)\n",
    "\n",
    "    def relu(self,X):        \n",
    "        return np.clip(X,0,6) # Relu6 implemented to avoid problems with +inf (ref Alex Krizhevsky article)\n",
    "    \n",
    "    def relu_derivative(self,X):\n",
    "        X[X<=0] = 0\n",
    "        X[X>0] = 1\n",
    "        return X\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        # Kaiming Weight Initialization\n",
    "        \n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / \\n\n",
    "                np.sqrt(2 / self.layers_size[l])\n",
    "                \n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    "            \n",
    "           # Initialization of paremeters used for Adams gradient descent \n",
    "            \n",
    "            self.parameters[\"firstW\" + str(l)] = 0\n",
    "            self.parameters[\"secondW\" + str(l)] = 0            \n",
    "            self.parameters[\"firstb\" + str(l)] = 0\n",
    "            self.parameters[\"secondb\" + str(l)] = 0\n",
    "              \n",
    " \n",
    "    def forward(self, X, activ_function):\n",
    "        \n",
    "# #######################################\n",
    "#  Implement the foward propagation.\n",
    "        # Argument:\n",
    "#     X -- input data of shape (number of examples,size of one example)\n",
    "#     activ_function -- activation function of the hidden layers\n",
    "\n",
    "#     Returns:\n",
    "#     A -- The results of the output layer\n",
    "#     store -- a dictionary containing \"W1\", \"W2\", \"Z1\", \"Z2\", \"A1\" and \"A2\" and so on\n",
    "        \n",
    "        store = {}\n",
    "        \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = activ_function(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z) # Output layer function\n",
    "        \n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    " \n",
    "        return A, store\n",
    "    \n",
    "    def adam(self,first,second,dx):\n",
    "# #######################################\n",
    "# Implement the Almost Adam optimization. Updates the first and second moment.\n",
    "# Argument:\n",
    "# first - first moment from step_(n-1)\n",
    "# second - second moment from step_(n-1)\n",
    "# Return:\n",
    "# first - first moment from step_n\n",
    "# second - second moment from step_n\n",
    "                 \n",
    "            first = self.beta1 * first + (1-self.beta1) * dx\n",
    "            second = self.beta2 * second + (1-self.beta2) * dx * dx               \n",
    "            \n",
    "            return first,second\n",
    "\n",
    "    \n",
    "    def backward(self, X, Y, store,activ_deriv):\n",
    "\n",
    "# #######################################\n",
    "#  Implement the backward propagation.\n",
    "\n",
    "#  Arguments:\n",
    "# activ_deriv -- derivative of the activation function chosen for the hidden layers \n",
    "# store -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\". Cache output from forward_propagation()\n",
    "# X -- input data of shape (number of examples,size of one example)\n",
    "# Y -- \"true\" labels vector of shape (number of examples,number of different classes)\n",
    "\n",
    "# Returns:\n",
    "# derivatives -- python dictionary containing your gradients with respect to different parameters\n",
    "# ##################################\n",
    "        \n",
    "        derivatives = {}\n",
    "        \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        \n",
    "        \n",
    "        dZ = A - Y.T  # Cross entropy derivative\n",
    "       \n",
    "        \n",
    "        # dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n + (self.Lambda/X.shape[0])*store[\"W\" +str(self.L)]\n",
    "        \n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    " \n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * activ_deriv(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T) + (self.Lambda/X.shape[0])*store[\"W\" +str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    " \n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y,activ_function,learning_rate = 1  , n_iterations=1000):\n",
    "# #######################################\n",
    "# fit function - fits the NN to the data training         \n",
    "# Argument: \n",
    "# X -- can be either only the X_test or a list containing X_train and X_test in this order.\n",
    "# Y -- can be either only the Y_test or a list containing Y_train and Y_test in this order.      \n",
    "# #######################################        \n",
    "        \n",
    "        \n",
    "        if activ_function == 'relu':\n",
    "            function = self.relu\n",
    "            deriv = self.relu_derivative\n",
    "        elif activ_function == 'sigmoid':\n",
    "            function = self.sigmoid\n",
    "            deriv = self.sigmoid_derivative\n",
    "        else:\n",
    "            print('Function not supported')\n",
    "            exit()\n",
    "        \n",
    "        if len(X) == 2: #Assumes that both train and test batches were given at once\n",
    "        # Else do nothing - only the train batch was given\n",
    "            train_x,test_x = X\n",
    "            train_y,test_y = Y\n",
    "            X = train_x\n",
    "            Y = train_y\n",
    "            \n",
    "        \n",
    "        learning_init = learning_rate\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.n = X.shape[0]\n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    "         \n",
    "        #\n",
    "        D = 10 #Number of learning rate restarts during training. Does not work with 0\n",
    "        self.cycle = np.array(range(round(n_iterations/D)))\n",
    "       \n",
    " \n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        \n",
    "        for loop in range(n_iterations):\n",
    "            \n",
    "            learning_rate = 0.5 * learning_init * (1 + np.cos(np.roll(self.cycle,-loop)[0] * np.pi / D)) \n",
    "            A, store = self.forward(X,function)\n",
    "            \n",
    "            L2_regularization = 0\n",
    "            \n",
    "            for l in range(1, self.L + 1):\n",
    "                \n",
    "                \n",
    "                L2_regularization = L2_regularization + np.sum(np.square(self.parameters[\"W\" + str(l)]))\n",
    "                \n",
    "                \n",
    "            L2_regularization = L2_regularization*self.Lambda*0.5/Y.shape[0]\n",
    "            \n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8)) + L2_regularization\n",
    "            derivatives = self.backward(X, Y, store,deriv)\n",
    "            \n",
    "            \n",
    "                \n",
    "            for l in range(1, self.L + 1):  \n",
    "                \n",
    "                \n",
    "                # Adam almost \n",
    "                # The lines below are difficult to read so here I write a concise example\n",
    "                # where the subscripts indicate the step count  \n",
    "                #  firstW1_(i+1),secondW1_(i+1) = adam(firstW1_i,secondW1_i,dW)\n",
    "                \n",
    "                self.parameters[\"firstW\" + str(l)],self.parameters[\"secondW\" + str(l)] = \\n\n",
    "                self.adam(self.parameters[\"firstW\" + str(l)],self.parameters[\"secondW\" + str(l)],derivatives[\"dW\" + str(l)])\n",
    "                \n",
    "                self.parameters[\"W\" + str(l)] -= learning_rate * self.parameters[\"firstW\" + str(l)] / \\n\n",
    "                np.sqrt(self.parameters[\"secondW\" +str(l)] + 1e-7) \n",
    "                                \n",
    "                self.parameters[\"firstb\" + str(l)],self.parameters[\"secondb\" + str(l)] = \\n\n",
    "                self.adam(self.parameters[\"firstb\" + str(l)],self.parameters[\"secondb\" + str(l)],derivatives[\"db\" + str(l)])\n",
    "                \n",
    "                self.parameters[\"b\" + str(l)] -= learning_rate * self.parameters[\"firstb\" + str(l)] / \\n\n",
    "                np.sqrt(self.parameters[\"secondb\" +str(l)] + 1e-7) \n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y,function))\n",
    "                # Restart for Learning rate\n",
    "                \n",
    "                \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    "                self.accuracy.append(self.predict(X, Y,function))\n",
    "                self.accuracy_test.append(self.predict(test_x, test_y,function))\n",
    "                \n",
    "    def predict(self, X, Y,function):\n",
    "        \n",
    "        A, cache = self.forward(X,function)\n",
    "        \n",
    "        # These two lines bellow do the oposite of the hot_encoder function\n",
    "        # transforms the output data with size (N,C) to (N,1) to evaluate the accuracy\n",
    "        \n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.title(\"Neural Network - \" + str(sum(layers_dims[1:-1])) + \" Neurons - \" + str(len(layers_dims)-1) + \" Layers\" )\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()        \n",
    "\n",
    "\n",
    "    def  plot_acc(self):        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.accuracy)), self.accuracy)\n",
    "        plt.title(\"Neural Network - \" + str(sum(layers_dims[1:-1])) + \" Neurons - \" + str(len(layers_dims)-1) + \" Layers\" )\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.show()     \n",
    "        \n",
    "    def plot(self):\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.title(\"Neural Network - \" + str(sum(layers_dims[1:-1])) + \" Neurons - \" + str(len(layers_dims)-1) + \" Layers\" )\n",
    "        plt.ylabel(\"cost\")\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(np.arange(len(self.accuracy)), self.accuracy)\n",
    "        plt.plot(np.arange(len(self.accuracy_test)), self.accuracy_test)\n",
    "        plt.legend(['Train','Test'])\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encoder(Y):\n",
    " # Changes the input Y with size (N,1) to a 2D matrix with size (N,C)\n",
    " # where N is the number of data and C the number of different classes\n",
    " # C = 10.  \n",
    "    C = 10 ;  \n",
    "    out = np.zeros((len(Y),C))\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "           \n",
    "        out[i,Y[i]] = 1\n",
    "        \n",
    "    return out\n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    "    \n",
    "    # Subtract the mean image from data.\n",
    "    train_x -= np.mean(train_x,axis=0)\n",
    "    test_x -= np.mean(test_x,axis=0)\n",
    "    \n",
    "    # Transform Data from size (N,1) to (N,C) where N is the number of data \n",
    "    # and C the number of different classes\n",
    "    train_y = hot_encoder(train_y)\n",
    "    test_y = hot_encoder(test_y)\n",
    " \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def lecture_cifar(path,batch):\n",
    "    X = np.empty((0,3072))\n",
    "    Y = np.empty((1,0),dtype=int)\n",
    "    if batch == 1:\n",
    "        \n",
    "        file = path + \n",
    " + str(batch)\n",
    "        \n",
    "        dictionary = unpickle(file)\n",
    "        X = dictionary[b'data']\n",
    "        Y = np.asarray(dictionary[b'labels'])\n",
    "           \n",
    "    else:\n",
    "            for i in range(1,batch+1):\n",
    "                 file = path + \n",
    " + str(i)        \n",
    "                 dictionary = unpickle(file)\n",
    "                 X = np.vstack([X,dictionary[b'data']])\n",
    "                 temp = np.asarray(dictionary[b'labels'])\n",
    "                 Y = np.append(Y,temp)\n",
    "                \n",
    "                 \n",
    "    X = np.float32(X)   \n",
    "    return X, Y\n",
    "\n",
    "def decoupage_donnees(X,Y):\n",
    "\n",
    "    training_indice = random.sample(range(len(X)),k=round(0.8*len(X)))\n",
    "    training_indice = sorted(np.asarray(training_indice))\n",
    "    \n",
    "    test_indice = np.setdiff1d(range(len(X)), training_indice)\n",
    "    \n",
    "    \n",
    "    Xapp = X[training_indice,:]\n",
    "    Yapp = Y[training_indice]\n",
    "    Yapp = np.reshape(Yapp, (len(Yapp), 1) )\n",
    "    Xtest = X[test_indice,:]\n",
    "    Ytest = Y[test_indice]\n",
    "    \n",
    "    return Xapp,Yapp,Xtest,Ytest    \n",
    "\n",
    "def minibatch (X,Y,N):\n",
    "\n",
    "    X = X[:N]    \n",
    "    Y = Y[:N]\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "def unflatten_image(img_flat):\n",
    "    img_R = img_flat[0:1024].reshape((32, 32))\n",
    "    img_G = img_flat[1024:2048].reshape((32, 32))\n",
    "    img_B = img_flat[2048:3072].reshape((32, 32))\n",
    "    img = np.dstack((img_R, img_G, img_B))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fad7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load a small minibatch and inspect shapes\n",
    "path = 'cifar-10-batches-py'  # adjust if your data directory differs\n",
    "batch = 1\n",
    "X, Y = lecture_cifar(path, batch)\n",
    "M = 500\n",
    "X, Y = minibatch(X, Y, M)\n",
    "Xapp, Yapp, Xtest, Ytest = decoupage_donnees(X, Y)\n",
    "print('Train shape, Train labels, Test shape, Test labels:', Xapp.shape, Yapp.shape, Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a4651",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- The notebook mirrors the code in `src/nn_classification.py`.\n",
    "- Adjust `path` to point to your `cifar-10-batches-py` folder before running the example cell.\n",
    "- The plotting methods reference `layers_dims` which are external; avoid calling them without defining that variable."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
