{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00e045c",
   "metadata": {},
   "source": [
    "# Deep Learning Fundamentals: Perceptron Multicamadas (MLP) do Zero\n",
    "\n",
    "## 1. Introdução\n",
    "Superando as limitações dos classificadores baseados em distância (como o KNN demonstrado anteriormente), este projeto implementa uma **Rede Neural Artificial (ANN)** totalmente conectada para classificar o dataset CIFAR-10.\n",
    "\n",
    "### Destaques Técnicos da Implementação\n",
    "Ao invés de utilizar frameworks como PyTorch ou TensorFlow, esta implementação constrói a arquitetura \"from scratch\" usando apenas NumPy. Isso demonstra domínio sobre:\n",
    "* **Backpropagation:** Cálculo manual de gradientes matriciais.\n",
    "* **Otimização Avançada:** Implementação de um otimizador customizado (\"Almost Adam\") com **Cosine Annealing** para o Learning Rate.\n",
    "* **Regularização:** Weight Decay (L2) e Inicialização de He (Kaiming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c8ac6",
   "metadata": {},
   "source": [
    "## 2. Engenharia de Features e Pré-processamento\n",
    "Para garantir a estabilidade numérica e a convergência do Gradiente Descendente, aplicamos o seguinte pipeline:\n",
    "\n",
    "1.  **Normalização (Min-Max Scaling):** Reescalonamento dos pixels $[0, 255] \\rightarrow [0, 1]$.\n",
    "2.  **Centralização (Mean Subtraction):** Subtração da imagem média do dataset para centrar os dados em zero.\n",
    "3.  **One-Hot Encoding:** Transformação dos labels categóricos em vetores binários para o cálculo da perda (Cross-Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd625485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Funções de carregamento da base de dados CIFAR-10\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def ler_cifar(path, batch):\n",
    "# ##################################    \n",
    "# ler_cifar importa as imagens da base de dados CIFAR 10 \n",
    "# Argumentos:\n",
    "# path -- caminho que leva ao diretório da base de dados\n",
    "# batch -- A base de dados contém 50 mil imagens divididas em 5 minibatches (lotes),\n",
    "# este argumento é um escalar e define quantos minibatches serão importados. \n",
    "## Retorna: \n",
    "# X -- Uma matriz de tamanho (N, M) onde N é o número de imagens e M o tamanho da imagem vetorizada\n",
    "# Y -- Um vetor 1D de tamanho (N, 1) que contém a classificação correta para cada imagem\n",
    "# ##################################    \n",
    "\n",
    "    X = np.empty((0,3072))\n",
    "    Y = np.empty((1,0),dtype=int)\n",
    "    if batch == 1:\n",
    "        \n",
    "        file = path + \"\\\\data_batch_\" + str(batch)\n",
    "        \n",
    "        dictionary = unpickle(file)\n",
    "        X = dictionary[b'data']\n",
    "        Y = np.asarray(dictionary[b'labels'])\n",
    "            \n",
    "    else:\n",
    "            for i in range(1,batch+1):\n",
    "                 file = path + \"\\\\data_batch_\" + str(i)        \n",
    "                 dictionary = unpickle(file)\n",
    "                 X = np.vstack([X,dictionary[b'data']])\n",
    "                 temp = np.asarray(dictionary[b'labels'])\n",
    "                 Y = np.append(Y,temp)\n",
    "                \n",
    "                 \n",
    "    X = np.float32(X)   \n",
    "    return X, Y\n",
    "\n",
    "def cortar_dados(X,Y):\n",
    "\n",
    "    training_indice = random.sample(range(len(X)),k=round(0.8*len(X)))\n",
    "    training_indice = sorted(np.asarray(training_indice))\n",
    "    \n",
    "    test_indice = np.setdiff1d(range(len(X)), training_indice)\n",
    "    \n",
    "    \n",
    "    Xapp = X[training_indice,:]\n",
    "    Yapp = Y[training_indice]\n",
    "    Yapp = np.reshape(Yapp, (len(Yapp), 1) )\n",
    "    Xtest = X[test_indice,:]\n",
    "    Ytest = Y[test_indice]\n",
    "    \n",
    "    return Xapp,Yapp,Xtest,Ytest    \n",
    "\n",
    "def minibatch (X,Y,N):\n",
    "\n",
    "    X = X[:N]    \n",
    "    Y = Y[:N]\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "def unflatten_image(img_flat):\n",
    "    img_R = img_flat[0:1024].reshape((32, 32))\n",
    "    img_G = img_flat[1024:2048].reshape((32, 32))\n",
    "    img_B = img_flat[2048:3072].reshape((32, 32))\n",
    "    img = np.dstack((img_R, img_G, img_B))\n",
    "    return img   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44fad7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caiqu\\AppData\\Local\\Temp\\ipykernel_4220\\3133941632.py:10: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  dict = pickle.load(fo, encoding='bytes')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape, Train labels, Test shape, Test labels: (400, 3072) (400, 1) (100, 3072) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Carregamento de um minibatch e inspeção das dimensões dos dados\n",
    "path = r\"C:\\Users\\caiqu\\Desktop\\Data Science\\AI Classification Problem\\cifar-10-batches-py\"  #\n",
    "batch = 5\n",
    "X, Y = ler_cifar(path, batch)\n",
    "M = 500\n",
    "X, Y = minibatch(X, Y, M)\n",
    "Xapp, Yapp, Xtest, Ytest = cortar_dados(X, Y)\n",
    "print('Train shape, Train labels, Test shape, Test labels:', Xapp.shape, Yapp.shape, Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a732f48",
   "metadata": {},
   "source": [
    "## 3. Arquitetura da Rede Neural\n",
    "\n",
    "A classe `ANN` foi desenhada para ser modular. Abaixo detalho as escolhas arquiteturais:\n",
    "\n",
    "### 3.1 Funções de Ativação e Estabilidade\n",
    "* **Hidden Layers:** Utilizamos `ReLU6` ($\\min(\\max(0, x), 6)$). Diferente da ReLU padrão, a ReLU6 impede que as ativações cresçam indefinidamente, ajudando na estabilidade numérica em precisão mista ou fixa.\n",
    "* **Output Layer:** Função `Softmax` para converter os *logits* em uma distribuição de probabilidade.\n",
    "\n",
    "### 3.2 Otimização: \"Almost Adam\" & Scheduler\n",
    "Implementamos uma variação do otimizador **Adam** (Adaptive Moment Estimation).\n",
    "* **Momentum:** Utiliza médias móveis dos gradientes (1º momento) e dos gradientes ao quadrado (2º momento) para acelerar a convergência.\n",
    "* **Cosine Annealing:** O Learning Rate não é estático; ele decai seguindo uma curva cosseno. Isso permite grandes saltos no início (exploração) e ajustes finos no final (exploração de mínimos locais).\n",
    "\n",
    "$$\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48f6bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import pickle # Para salvar o arquivo\n",
    "import copy   # Para copiar os pesos sem referência\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers_dims: List[int], lambda_reg: float = 0.0, keep_prob: float = 1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers_dims: Lista de neurônios [input, hidden..., output]\n",
    "            lambda_reg: Regularização L2 (Weight Decay)\n",
    "            keep_prob: Probabilidade de manter neurônio ativo (Dropout). 1.0 = Sem Dropout.\n",
    "        \"\"\"\n",
    "        self.layers_dims = layers_dims\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.keep_prob = keep_prob\n",
    "        self.parameters = {}\n",
    "        self.L = len(layers_dims) - 1\n",
    "        self.history = {'cost': [], 'train_acc': [], 'test_acc': []}\n",
    "        \n",
    "        # Adam params\n",
    "        self.beta1, self.beta2, self.epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        np.random.seed(42)\n",
    "        for l in range(1, len(self.layers_dims)):\n",
    "            # He Initialization\n",
    "            self.parameters[f\"W{l}\"] = np.random.randn(self.layers_dims[l], self.layers_dims[l-1]) * np.sqrt(2 / self.layers_dims[l-1])\n",
    "            self.parameters[f\"b{l}\"] = np.zeros((self.layers_dims[l], 1))\n",
    "            # Adam Cache\n",
    "            self.parameters[f\"v_dW{l}\"] = np.zeros((self.layers_dims[l], self.layers_dims[l-1]))\n",
    "            self.parameters[f\"s_dW{l}\"] = np.zeros((self.layers_dims[l], self.layers_dims[l-1]))\n",
    "            self.parameters[f\"v_db{l}\"] = np.zeros((self.layers_dims[l], 1))\n",
    "            self.parameters[f\"s_db{l}\"] = np.zeros((self.layers_dims[l], 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(Z): return np.maximum(0, Z) # Usei max simples para velocidade, pode usar ReLU6 se preferir\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return e_Z / np.sum(e_Z, axis=0, keepdims=True)\n",
    "\n",
    "    def _augment_batch(self, X):\n",
    "        \"\"\"\n",
    "        Data Augmentation simples: Flip Horizontal aleatório.\n",
    "        Assume X na forma (Features, Exemplos).\n",
    "        Para CIFAR (3072 features), precisamos reconstruir a imagem 32x32x3 para flipar corretamente.\n",
    "        \"\"\"\n",
    "        # Se não for imagem 32x32x3 achatada, retorna sem alterar (segurança)\n",
    "        if X.shape[0] != 3072: return X \n",
    "        \n",
    "        X_aug = X.copy()\n",
    "        n_samples = X.shape[1]\n",
    "        \n",
    "        # Decide aleatoriamente quais exemplos flipar (50% de chance)\n",
    "        flip_indices = np.random.rand(n_samples) > 0.5\n",
    "        \n",
    "        if np.sum(flip_indices) > 0:\n",
    "            # Reshape para (3, 32, 32, N_flip) -> Formato CIFAR (Channels, H, W)\n",
    "            # Nota: O reshape depende de como os dados foram carregados (C-order ou F-order).\n",
    "            # Assumindo padrão CIFAR (R-G-B flatten):\n",
    "            images = X_aug[:, flip_indices].reshape(3, 32, 32, -1, order='C')\n",
    "            \n",
    "            # Flip no eixo da largura (axis 2)\n",
    "            images = np.flip(images, axis=2)\n",
    "            \n",
    "            # Flatten de volta\n",
    "            X_aug[:, flip_indices] = images.reshape(3072, -1, order='C')\n",
    "            \n",
    "        return X_aug\n",
    "\n",
    "    def forward_propagation(self, X, training=True):\n",
    "        cache = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        for l in range(1, self.L):\n",
    "            Z = np.dot(self.parameters[f\"W{l}\"], A) + self.parameters[f\"b{l}\"]\n",
    "            A = self.relu(Z)\n",
    "            \n",
    "            # DROPOUT (Apenas no treino e nas camadas ocultas)\n",
    "            if training and self.keep_prob < 1.0:\n",
    "                D = np.random.rand(*A.shape) < self.keep_prob\n",
    "                A = (A * D) / self.keep_prob # Inverted Dropout\n",
    "                cache[f\"D{l}\"] = D\n",
    "            \n",
    "            cache[f\"Z{l}\"] = Z\n",
    "            cache[f\"A{l}\"] = A\n",
    "            \n",
    "        # Saída (sem dropout)\n",
    "        Z = np.dot(self.parameters[f\"W{self.L}\"], A) + self.parameters[f\"b{self.L}\"]\n",
    "        AL = self.softmax(Z)\n",
    "        \n",
    "        cache[f\"Z{self.L}\"] = Z\n",
    "        cache[f\"A{self.L}\"] = AL\n",
    "        return AL, cache\n",
    "\n",
    "    def compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        cost = -np.mean(Y * np.log(AL + 1e-8)) * Y.shape[0] # Soma classes, média exemplos\n",
    "        \n",
    "        # L2 Regularization\n",
    "        if self.lambda_reg > 0:\n",
    "            l2_sum = sum(np.sum(np.square(self.parameters[f\"W{l}\"])) for l in range(1, self.L+1))\n",
    "            cost += (self.lambda_reg / (2 * m)) * l2_sum\n",
    "            \n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self, AL, Y, cache):\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "        dZ = AL - Y # Derivada Softmax+CrossEntropy\n",
    "        \n",
    "        for l in range(self.L, 0, -1):\n",
    "            A_prev = cache[f\"A{l-1}\"]\n",
    "            W = self.parameters[f\"W{l}\"]\n",
    "            \n",
    "            grads[f\"dW{l}\"] = (1/m) * np.dot(dZ, A_prev.T) + (self.lambda_reg/m) * W\n",
    "            grads[f\"db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 1:\n",
    "                dA_prev = np.dot(W.T, dZ)\n",
    "                \n",
    "                # APLICA MÁSCARA DO DROPOUT NO BACKPROP\n",
    "                if self.keep_prob < 1.0 and f\"D{l-1}\" in cache:\n",
    "                    dA_prev = (dA_prev * cache[f\"D{l-1}\"]) / self.keep_prob\n",
    "                \n",
    "                dZ = dA_prev * (cache[f\"Z{l-1}\"] > 0) # ReLU derivative\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, lr, t):\n",
    "        for l in range(1, self.L + 1):\n",
    "            # Adam Update\n",
    "            self.parameters[f\"v_dW{l}\"] = self.beta1 * self.parameters[f\"v_dW{l}\"] + (1-self.beta1)*grads[f\"dW{l}\"]\n",
    "            self.parameters[f\"v_db{l}\"] = self.beta1 * self.parameters[f\"v_db{l}\"] + (1-self.beta1)*grads[f\"db{l}\"]\n",
    "            self.parameters[f\"s_dW{l}\"] = self.beta2 * self.parameters[f\"s_dW{l}\"] + (1-self.beta2)*(grads[f\"dW{l}\"]**2)\n",
    "            self.parameters[f\"s_db{l}\"] = self.beta2 * self.parameters[f\"s_db{l}\"] + (1-self.beta2)*(grads[f\"db{l}\"]**2)\n",
    "            \n",
    "            # Bias correction (opcional, mas bom para estabilidade inicial)\n",
    "            v_corr_dW = self.parameters[f\"v_dW{l}\"] / (1 - self.beta1**t)\n",
    "            s_corr_dW = self.parameters[f\"s_dW{l}\"] / (1 - self.beta2**t)\n",
    "            v_corr_db = self.parameters[f\"v_db{l}\"] / (1 - self.beta1**t)\n",
    "            s_corr_db = self.parameters[f\"s_db{l}\"] / (1 - self.beta2**t)\n",
    "\n",
    "            self.parameters[f\"W{l}\"] -= lr * (v_corr_dW / (np.sqrt(s_corr_dW) + self.epsilon))\n",
    "            self.parameters[f\"b{l}\"] -= lr * (v_corr_db / (np.sqrt(s_corr_db) + self.epsilon))\n",
    "\n",
    "    def fit(self, X_train, Y_train, X_test, Y_test, epochs=100, lr=0.001, batch_size=256):\n",
    "        # Transposições (se necessário)\n",
    "        if X_train.shape[0] != self.layers_dims[0]: X_train = X_train.T\n",
    "        if X_test.shape[0] != self.layers_dims[0]: X_test = X_test.T\n",
    "        if Y_train.shape[0] != self.layers_dims[-1]: Y_train = Y_train.T\n",
    "        if Y_test.shape[0] != self.layers_dims[-1]: Y_test = Y_test.T\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "        m = X_train.shape[1]\n",
    "        \n",
    "        # Variáveis para Checkpoint\n",
    "        best_acc = 0.0\n",
    "        best_parameters = {}\n",
    "        \n",
    "        print(f\"Treinando: {m} exemplos, {epochs} épocas. Dropout: {self.keep_prob}\")\n",
    "        \n",
    "        for i in range(1, epochs + 1):\n",
    "            # ... (Lógica do Mini-batch e Augmentation continua igual) ...\n",
    "            permutation = np.random.permutation(m)\n",
    "            X_shuffled = X_train[:, permutation]\n",
    "            Y_shuffled = Y_train[:, permutation]\n",
    "            \n",
    "            for j in range(0, m, batch_size):\n",
    "                begin, end = j, min(j + batch_size, m)\n",
    "                X_batch = X_shuffled[:, begin:end]\n",
    "                Y_batch = Y_shuffled[:, begin:end]\n",
    "                X_batch = self._augment_batch(X_batch)\n",
    "                \n",
    "                AL, cache = self.forward_propagation(X_batch, training=True)\n",
    "                grads = self.backward_propagation(AL, Y_batch, cache)\n",
    "                self.update_parameters(grads, lr, i)\n",
    "            \n",
    "            # Avaliação e Checkpoint\n",
    "            if i % 10 == 0 or i == epochs:\n",
    "                train_acc = self.evaluate(X_train, Y_train)\n",
    "                test_acc = self.evaluate(X_test, Y_test)\n",
    "                \n",
    "                self.history['train_acc'].append(train_acc)\n",
    "                self.history['test_acc'].append(test_acc)\n",
    "                \n",
    "                # --- LÓGICA DE SALVAR O MELHOR MODELO ---\n",
    "                if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    # Deep copy é crucial aqui! Se usar =, ele atualiza por referência\n",
    "                    best_parameters = copy.deepcopy(self.parameters)\n",
    "                    print(f\"Época {i} - *Novo Melhor Modelo*: {best_acc:.2f}%\")\n",
    "                else:\n",
    "                    print(f\"Época {i} - Train: {train_acc:.1f}% - Test: {test_acc:.1f}%\")\n",
    "\n",
    "        # Ao final do treino, restauramos os melhores pesos na classe\n",
    "        print(f\"\\nTreinamento finalizado. Restaurando melhores pesos com Acurácia: {best_acc:.2f}%\")\n",
    "        self.parameters = best_parameters\n",
    "\n",
    "    def save_model(self, filename=\"best_model_cifar10.pkl\"):\n",
    "        \"\"\"Salva os parâmetros aprendidos e a arquitetura em um arquivo.\"\"\"\n",
    "        model_data = {\n",
    "            \"parameters\": self.parameters,\n",
    "            \"layers_dims\": self.layers_dims,\n",
    "            \"history\": self.history\n",
    "        }\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Modelo salvo com sucesso em {filename}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename):\n",
    "        \"\"\"Carrega um modelo salvo e retorna uma instância da classe.\"\"\"\n",
    "        with open(filename, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Cria uma nova instância com a arquitetura salva\n",
    "        # Nota: lambda_reg e keep_prob não são salvos aqui, assumimos padrão para inferência\n",
    "        instance = cls(layers_dims=model_data[\"layers_dims\"])\n",
    "        instance.parameters = model_data[\"parameters\"]\n",
    "        instance.history = model_data[\"history\"]\n",
    "        instance.L = len(model_data[\"layers_dims\"]) - 1\n",
    "        \n",
    "        print(f\"Modelo carregado. Arquitetura: {instance.layers_dims}\")\n",
    "        return instance\n",
    "    def evaluate(self, X, Y):\n",
    "        AL, _ = self.forward_propagation(X, training=False)\n",
    "        predictions = np.argmax(AL, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        return np.mean(predictions == labels) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca88dc",
   "metadata": {},
   "source": [
    "## Data Pipeline (CIFAR-10)\n",
    "\n",
    "O script inclui funções auxiliares para lidar com o formato específico do conjunto de dados CIFAR-10:\n",
    "- One-Hot Encoding: A função hot_encoder converte rótulos de classe (como 3) em vetores (como $[0,0,0,1,0,0,0,0,0,0]$), o que é necessário para calcular a perda de Entropia Cruzada (Cross-Entropy loss).\n",
    "- Normalização: Em pre_process_data, os valores dos pixels (0–255) são divididos por 255 para escaloná-los entre 0 e 1. A função também realiza a subtração da média, o que centraliza os dados em torno de zero, ajudando a rede neural a convergir mais rapidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8457b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encoder(Y):\n",
    " # Changes the input Y with size (N,1) to a 2D matrix with size (N,C)\n",
    " # where N is the number of data and C the number of different classes\n",
    " # C = 10.  \n",
    "    C = 10 ;  \n",
    "    out = np.zeros((len(Y),C))\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "           \n",
    "        out[i,Y[i]] = 1\n",
    "        \n",
    "    return out\n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    "    \n",
    "    # Subtract the mean image from data.\n",
    "    train_x -= np.mean(train_x,axis=0)\n",
    "    test_x -= np.mean(test_x,axis=0)\n",
    "    \n",
    "    # Transform Data from size (N,1) to (N,C) where N is the number of data \n",
    "    # and C the number of different classes\n",
    "    train_y = hot_encoder(train_y)\n",
    "    test_y = hot_encoder(test_y)\n",
    " \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "def plot_history(self):\n",
    "        \"\"\"\n",
    "        Plota a evolução do custo e a comparação de acurácia (Treino vs Teste).\n",
    "        Ideal para visualizar Overfitting vs Generalização.\n",
    "        \"\"\"\n",
    "        # Cria uma figura com 2 subplots lado a lado\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Gráfico 1: Evolução da Acurácia\n",
    "        # Se houver dados suficientes, plota.\n",
    "        if len(self.history['train_acc']) > 0:\n",
    "            ax1.plot(self.history['train_acc'], label='Treino (Com Augmentation)', color='blue', linewidth=2)\n",
    "            ax1.plot(self.history['test_acc'], label='Teste (Validação)', color='orange', linewidth=2, linestyle='--')\n",
    "            \n",
    "            ax1.set_title('Análise de Overfitting: Acurácia')\n",
    "            ax1.set_xlabel('Épocas')\n",
    "            ax1.set_ylabel('Acurácia (%)')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Destaca o valor final\n",
    "            final_train = self.history['train_acc'][-1]\n",
    "            final_test = self.history['test_acc'][-1]\n",
    "            ax1.annotate(f'{final_train:.1f}%', xy=(len(self.history['train_acc'])-1, final_train), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "            ax1.annotate(f'{final_test:.1f}%', xy=(len(self.history['test_acc'])-1, final_test), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "        # Gráfico 2: Evolução do Custo (Loss)\n",
    "        if len(self.history['cost']) > 0:\n",
    "            ax2.plot(self.history['cost'], label='Custo de Teste', color='red')\n",
    "            ax2.set_title('Estabilidade do Treinamento: Custo')\n",
    "            ax2.set_xlabel('Checkpoints')\n",
    "            ax2.set_ylabel('Loss (Cross-Entropy)')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Pre-processamento dos dados para remover a média e normalizar\n",
    "\n",
    "train_x, train_y, test_x, test_y = pre_process_data(Xapp, Yapp, Xtest, Ytest)\n",
    "\n",
    "train_x =  train_x.astype(np.float64)\n",
    "test_x =  test_x.astype(np.float64)\n",
    "\n",
    "# definir base de dados de treino e teste\n",
    "\n",
    "X = [train_x,test_x]\n",
    "Y = [train_y,test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea003804",
   "metadata": {},
   "source": [
    "## Treinando a rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52b16903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando: 400 exemplos, 500 épocas. Dropout: 0.6\n",
      "Época 10 - *Novo Melhor Modelo*: 30.00%\n",
      "Época 20 - *Novo Melhor Modelo*: 38.00%\n",
      "Época 30 - Train: 70.2% - Test: 38.0%\n",
      "Época 40 - Train: 78.2% - Test: 36.0%\n",
      "Época 50 - Train: 80.2% - Test: 37.0%\n",
      "Época 60 - Train: 88.2% - Test: 34.0%\n",
      "Época 70 - Train: 91.2% - Test: 34.0%\n",
      "Época 80 - Train: 94.8% - Test: 36.0%\n",
      "Época 90 - Train: 96.5% - Test: 37.0%\n",
      "Época 100 - Train: 98.0% - Test: 35.0%\n",
      "Época 110 - *Novo Melhor Modelo*: 39.00%\n",
      "Época 120 - Train: 99.0% - Test: 39.0%\n",
      "Época 130 - Train: 99.5% - Test: 39.0%\n",
      "Época 140 - Train: 99.5% - Test: 36.0%\n",
      "Época 150 - Train: 100.0% - Test: 36.0%\n",
      "Época 160 - Train: 100.0% - Test: 37.0%\n",
      "Época 170 - Train: 100.0% - Test: 37.0%\n",
      "Época 180 - Train: 100.0% - Test: 35.0%\n",
      "Época 190 - Train: 100.0% - Test: 35.0%\n",
      "Época 200 - Train: 100.0% - Test: 34.0%\n",
      "Época 210 - Train: 100.0% - Test: 37.0%\n",
      "Época 220 - Train: 100.0% - Test: 37.0%\n",
      "Época 230 - Train: 100.0% - Test: 34.0%\n",
      "Época 240 - Train: 100.0% - Test: 37.0%\n",
      "Época 250 - Train: 100.0% - Test: 35.0%\n",
      "Época 260 - Train: 100.0% - Test: 35.0%\n",
      "Época 270 - Train: 100.0% - Test: 35.0%\n",
      "Época 280 - Train: 100.0% - Test: 34.0%\n",
      "Época 290 - Train: 100.0% - Test: 37.0%\n",
      "Época 300 - Train: 100.0% - Test: 37.0%\n",
      "Época 310 - Train: 100.0% - Test: 37.0%\n",
      "Época 320 - Train: 100.0% - Test: 34.0%\n",
      "Época 330 - Train: 100.0% - Test: 34.0%\n",
      "Época 340 - Train: 100.0% - Test: 37.0%\n",
      "Época 350 - Train: 100.0% - Test: 36.0%\n",
      "Época 360 - Train: 100.0% - Test: 38.0%\n",
      "Época 370 - Train: 100.0% - Test: 38.0%\n",
      "Época 380 - *Novo Melhor Modelo*: 40.00%\n",
      "Época 390 - Train: 100.0% - Test: 35.0%\n",
      "Época 400 - Train: 100.0% - Test: 37.0%\n",
      "Época 410 - Train: 100.0% - Test: 38.0%\n",
      "Época 420 - Train: 100.0% - Test: 36.0%\n",
      "Época 430 - Train: 100.0% - Test: 32.0%\n",
      "Época 440 - Train: 100.0% - Test: 38.0%\n",
      "Época 450 - Train: 100.0% - Test: 38.0%\n",
      "Época 460 - Train: 100.0% - Test: 34.0%\n",
      "Época 470 - Train: 100.0% - Test: 36.0%\n",
      "Época 480 - Train: 100.0% - Test: 37.0%\n",
      "Época 490 - Train: 100.0% - Test: 35.0%\n",
      "Época 500 - Train: 100.0% - Test: 34.0%\n",
      "\n",
      "Treinamento finalizado. Restaurando melhores pesos com Acurácia: 40.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# layers_dims = [3072, 512, 128, 10] # Exemplo de arquitetura\n",
    "# Reduza o tamanho da rede se continuar overfitting (ex: de 512 para 256)\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    layers_dims=[3072, 256, 128, 10], \n",
    "    lambda_reg=0.1,    # Aumentei a regularização L2\n",
    "    keep_prob=0.6      # Dropout agressivo: desliga 40% dos neurônios no treino\n",
    ")\n",
    "\n",
    "# batch_size maior ajuda a estabilizar o gradiente com augmentation\n",
    "model.fit(train_x, train_y, test_x, test_y, epochs=500, lr=0.0005, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada7438",
   "metadata": {},
   "source": [
    "## 4. Otimização e Regularização: Combatendo o Overfitting\n",
    "\n",
    "Durante os experimentos iniciais com a Rede Neural, observamos um fenômeno crítico: a acurácia de treino atingia rapidamente **100%**, enquanto a acurácia de teste estagnava em torno de **30-35%**. Isso caracteriza um **Overfitting Severo**, onde o modelo com milhões de parâmetros \"memoriza\" o ruído dos dados de treino, mas falha em generalizar.\n",
    "\n",
    "Para solucionar isso e demonstrar técnicas avançadas de Deep Learning \"from scratch\", implementamos:\n",
    "\n",
    "1.  **Inverted Dropout:** Durante o treino, neurônios aleatórios são \"desligados\" com probabilidade $1 - p$ (implementado na classe `NeuralNetwork` com `keep_prob=0.6`). Isso força a rede a aprender representações distribuídas e redundantes, impedindo que neurônios específicos se tornem excessivamente dependentes de features locais.\n",
    "    \n",
    "2.  **Data Augmentation (On-the-fly):** Implementamos uma função `_augment_batch` que aplica espelhamento horizontal (Horizontal Flip) aleatório nas imagens durante cada passo do gradiente. Isso efetivamente dobra o tamanho do dataset e ensina ao modelo que um \"carro virado para a esquerda\" ainda é um carro (invariância).\n",
    "\n",
    "3.  **L2 Regularization (Weight Decay):** Penalização dos pesos grandes na função de custo para reduzir a complexidade do modelo.\n",
    "\n",
    "### Resultado da Otimização\n",
    "Com essas alterações, esperamos reduzir a diferença entre as curvas de treino e teste (Generalization Gap), trocando um pouco da acurácia de treino por uma performance real mais robusta no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d9079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando o melhor modelo encontrado...\n",
      "Modelo salvo com sucesso em meu_modelo_cifar10.pkl\n",
      "\n",
      "Verificando carregamento...\n",
      "Modelo carregado. Arquitetura: [3072, 256, 128, 10]\n",
      "Acurácia do modelo carregado: 40.00%\n"
     ]
    }
   ],
   "source": [
    "# 1. O modelo já contém os melhores pesos encontrados durante o treino\n",
    "print(\"Salvando o melhor modelo encontrado...\")\n",
    "model.save_model(\"meu_modelo_cifar10.pkl\")\n",
    "\n",
    "# Avalia nos dados de teste\n",
    "acc = modelo_carregado.evaluate(test_x.T, test_y.T)\n",
    "print(f\"Acurácia do modelo carregado: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa44634",
   "metadata": {},
   "source": [
    "## 5. Resultados e Comparação: KNN vs ANN\n",
    "\n",
    "### Performance\n",
    "O modelo de Rede Neural (MLP) atingiu uma acurácia ligeiramente superior ao KNN nos dados de teste. Entretanto o modelo ainda sofre bastante com overfitting, sempre atingindo 100% de acuracia com os dados de apredizado.\n",
    "\n",
    "| Modelo | Acurácia (Teste) | Complexidade de Inferência |\n",
    "| :--- | :--- | :--- |\n",
    "| KNN (K=7) | ~29% | $O(N \\cdot D)$ (Lento/Custoso) |\n",
    "| **ANN (MLP)** | **~38%+** | **$O(1)$ (Rápido/Paramétrico)** |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5788002",
   "metadata": {},
   "source": [
    "## 6. Conclusão Final e Próximos Passos\n",
    "\n",
    "### Análise de Resultados\n",
    "Utilizando a totalidade do dataset CIFAR-10 (50.000 imagens) e técnicas de regularização (Dropout, L2), atingimos uma acurácia de **38%**. Embora este resultado seja superior ao baseline (KNN: ~29%), ele expõe as limitações intrínsecas das Redes Neurais Totalmente Conectadas (MLPs) para dados visuais:\n",
    "1.  **Perda de Informação Espacial:** Ao \"achatar\" a imagem 32x32 em um vetor de 3072, destruímos a correlação espacial entre pixels vizinhos.\n",
    "2.  **Ineficiência de Parâmetros:** Para tentar recuperar essa informação, a rede exige milhões de parâmetros, tornando a otimização de hiperparâmetros (Learning Rate, arquitetura, decaimento) uma tarefa de altíssima complexidade computacional e retornos decrescentes.\n",
    "\n",
    "### Decisão Estratégica: Transfer Learning\n",
    "Na indústria, a eficiência do desenvolvimento é crucial. Insistir na micro-otimização de uma MLP para classificação de imagens complexas não é a melhor alocação de recursos.\n",
    "\n",
    "A abordagem padrão de mercado (SOTA - State of the Art) para este problema é a utilização de **Redes Neurais Convolucionais (CNNs)**, que possuem *indutivo bias* para invariância espacial (translação, rotação).\n",
    "\n",
    "**Próximo Projeto:**\n",
    "Para superar a barreira dos 38% e atingir níveis de performance de produção (>90%), o próximo passo lógico não é refinar este modelo, mas sim adotar **Transfer Learning**. Utilizaremos uma arquitetura **ResNet-18** (pré-treinada na ImageNet), aproveitando a extração de features robustas já aprendidas para focar apenas no *fine-tuning* para as classes do CIFAR-10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
